\section{Auth-Service: Geräteregistrierung und HMAC-Authentifizierung}
Der Auth-Service implementiert die sichere Registrierung und Anmeldung von Controllern mittels eines Challenge-Response-Verfahrens auf Basis eines vorab geteilten Geheimnisses (PSK). Dieser Dienst ist als Flask-Webanwendung realisiert und stellt HTTP-Routen für die Controller-Initialisierung, -Verifikation sowie eine Admin-Registrierung bereit. Er bildet damit das sicherheitskritische Bindeglied zwischen neuen Geräten und der Systeminfrastruktur: 
Geräte erhalten hier ihre individuellen Messaging-Zugangsdaten, sofern sie einen kryptographischen Besitznachweis erbringen. Im Folgenden werden die Architektur und der Ablauf des Auth-Service beschrieben, gefolgt von besonderen Implementierungsdetails wie der PSK-Überprüfung, Wiederanlaufbarkeit und der automatischen Broker-Konfiguration.
\subsection{Architektur und persistente Datenhaltung}
Der Auth-Service ist als Flask-Applikation mit dokumentierter REST-API (via Swagger/Flasgger) umgesetzt. Kern der Implementierung ist eine zentrale Konfigurationsdatei (auth\_config.json), die folgende Informationen persistent speichert:

\begin{enumerate}
    \item Authorized Controllers: eine Liste aller registrierten Controller mit deren Controller-ID (eindeutige Kennung), zugehörigem PSK (token) sowie einem Hash dieses Tokens (token\_hash). Zusätzlich werden Metadaten wie Modell, Besitzer (Benutzername) und Beschreibung gespeichert. Diese Datei dient als kleine lokale Datenbank, um bereits registrierte Geräte und ihre Geheimnisse nachzuhalten.
    \item Active Challenges: temporäre Herausforderungen (Challenges) im laufenden Authentifizierungsprozess.
    Für jeden angeforderten Auth-Vorgang wird ein zufälliger Challenge-String erzeugt und unter dem Schlüssel des token\_hash zwischengespeichert.
    Dies ermöglicht es, eingehende Antworten eindeutig der zuvor ausgegebenen Challenge zuzuordnen, selbst bei parallelen Anfragen.
    \item Solace Credentials: bereits für Controller erstellte Zugangs-Credentials für den Message Broker (Solace). Pro Gerät wird hier der erzeugte Broker-Benutzername und das Passwort abgelegt, um bei wiederholter Authentifizierung nicht erneut einen Broker-Account anlegen zu müssen.
\end{enumerate}
Die Konfigurationsdaten werden auf dem Container-Dateisystem unter /config/auth\_config.json verwaltet und durch Volume-Mounting persistent gehalten. Dadurch bleiben Registrierung und vergebene Credentials auch beim Neustart des Dienstes erhalten – ein wesentlicher Aspekt für Wiederanlaufbarkeit. Ergänzend hält der Auth-Service eine Datenbankverbindung (PostgreSQL via psycopg2) bereit, um neue Controller auch in der zentralen Systemdatenbank (sensora) zu registrieren. In der Tabelle sensora.controllers werden u.a. Device-ID, Modell, Besitzer und ein vom System generierter geheimer Schlüssel abgelegt. Letzterer unterscheidet sich vom PSK und dient ggf. anderen Zwecken (z.B. der Kommunikation mit externen Anwendungen), ohne das eigentliche PSK offenzulegen. Die doppelte Ablage (Datei und DB) der Registrierungsdaten mag auf den ersten Blick redundant erscheinen, ermöglicht jedoch sowohl schnelle lokale Zugriffe während des Auth-Handshakes als auch die Integration ins relationale Gesamtdatenmodell.

\subsection{Ablauf der Geräte-Registrierung und Authentifizierung}
Der Auth-Service bietet drei Haupt-Endpoints, die den Lebenszyklus eines Geräts abbilden: (a) Registrierung eines neuen Controllers (nur Admin), (b) Initialisierung des Authentifizierungs-Handshakes durch das Gerät und (c) Verifikation der Challenge-Response. 

Der Authentifizierungsablauf erfolgt in mehreren Schritten:
\begin{enumerate}
    \item Admin-Registrierung: Zunächst muss ein neuer Controller in das System aufgenommen werden. Ein Administrator (oder ein automatisierter Setup-Prozess) ruft dazu den Endpoint /api/admin/controller auf und übermittelt zumindest den gewünschten Besitzer (username) sowie optional eine vorgegebene Controller-ID und Modellbeschreibung. Der Request ist durch einen speziellen Admin-API-Key (Header X-Admin-Key) geschützt, sodass nur berechtigte Instanzen Geräte hinzufügen können. Bei Aufruf generiert der Service serverseitig ein zufälliges Token (PSK) für das Gerät (hier als UUID v4). Zusätzlich wird ein Token-Hash berechnet: mittels HMAC-SHA256 über das Token mit einem globalen Server-Geheimnis (TOKEN\_SECRET). Dieser Hash dient als öffentlicher Identifikator des Geräts, der das eigentliche Token nicht preisgibt. Anschließend werden die Gerätedaten in der auth\_config.json persistiert (authorized\_controllers) und ein Eintrag in der DB-Tabelle controllers erzeugt. Der Response an den Admin enthält Controller-ID, Token und Token-Hash, welche sicher an das Gerät für die Inbetriebnahme weitergegeben werden (z.B. manuell oder per Provisioning-App). Dieser einmalige Out-of-Band-Schritt stellt sicher, dass jedes Gerät ein individuelles geheimes Token besitzt, das dem Server bekannt ist.
    \item Challenge-Anforderung (Gerät → Auth-Service): Hat das Gerät vom Admin sein Token erhalten und wird erstmals online genommen, initiiert es den Authentifizierungsprozess über den Endpoint /api/controller/init. Dabei sendet das Gerät nur den Hash seines Tokens (token\_hash) im Request – das eigentliche Token bleibt geheim und wird nie direkt übertragen. Der Auth-Service prüft den Request und generiert eine kryptographisch zufällige Challenge (16 Byte Hex-String via secrets.token\_hex) als Antwort. Diese Challenge wird im Server-Konfigurationsspeicher unter active\_challenges zusammen mit einem Zeitstempel abgelegt, indiziert durch den token\_hash. Die Response an das Gerät enthält die Challenge als JSON. Aus Sicherheitsgründen findet an dieser Stelle noch keine Verifizierung statt – auch ein unbekannter Token-Hash erhält (vorläufig) eine Challenge. Der eigentliche Abgleich erfolgt erst im nächsten Schritt. Dieses zweistufige Verfahren erhöht die Sicherheit, da ein Angreifer ohne Besitz des PSK aus der Challenge alleine keinen Zugang erlangt. Wichtig ist, dass jede Challenge unvorhersehbar und einmalig ist, um Replay-Angriffe auszuschließen. Der Auth-Service stellt dies durch Verwendung eines Kryptografie-Moduls sicher (Python secrets \cite{pythonSecrets} bietet laut Dokumentation einen sicheren Zufallszahlengenerator für solche Token).
    \item Challenge-Response (Gerät → Auth-Service): Nach Erhalt der Challenge berechnet das Gerät die Antwort: Es verwendet sein geheimes Token und wendet darauf die gleiche Hash-Funktion an, die auch der Server kennt. Im Code wird dazu HMAC-SHA256 genutzt, wobei das Token als Schlüssel und der Challenge-String als Nachricht dient. Das Ergebnis ist die Challenge-Response (Hex-String). Diese Antwort schickt das Gerät zurück an den Auth-Service, zusammen mit seinem token\_hash (zur Identifikation der Challenge) und meist einem Benutzernamen oder Kontoinformationen des Eigentümers. Der Auth-Service schlägt nun in seiner Konfiguration den Eintrag zum token\_hash nach: Dort findet er das ursprünglich registrierte Token (PSK) und die zugehörige Controller-ID. Sollte kein Eintrag existieren, wird der Prozess abgebrochen – das Gerät war nie registriert oder der Hash unbekannt (Fehler 403). Andernfalls vergleicht der Server die vom Gerät gesendete HMAC-Antwort mit dem erwarteten Wert, den er selbst berechnet (hmac.compare\_digest() verhindert Timing-Angriffe bei der String-Bewertung). Stimmen Response und eigener Wert überein, ist bewiesen, dass das Gerät das geheime Token besitzt und somit authentisch ist. Dieses Challenge-Response-Verfahren stellt eine sichere Authentifizierung dar\cite{hmacRFC}, ohne das PSK selbst über das Netzwerk zu senden. Ein abgehörter Challenge/Response-Wert kann später nicht wiederverwendet werden, da bei der nächsten Anmeldung eine andere Challenge zum Einsatz kommt (kein statisches Passwort)
    \item Broker-Zugangsdaten erstellen: Nach erfolgreicher Verifizierung entfernt der Auth-Service die verwendete Challenge (Verbrauch der einmaligen Challenge) und fährt mit der Provisionierung des Geräts für das Messaging-System fort. Hier greift eine weitere wichtige Implementierungsentscheidung: Der Service konfiguriert den Solace-MQTT-Broker dynamisch über dessen Management-API (SEMP v2). Konkret wird geprüft, ob für den Controller bereits Broker-Zugangsdaten in solace\_credentials vorliegen. Ist dies der erste erfolgreiche Auth-Vorgang für dieses Gerät, generiert der Service mittels der Hilfsfunktion create\_solace\_user(controller\_id) einen dedizierten Broker-Account:
    \begin{enumerate}
        \item Es wird ein eindeutiger Client-Username für den Controller erstellt (z.B. controller\_ab12... gekürzt auf Basis der ID) und ein zufälliges Passwort (secrets.token\_urlsafe(32)).
        \item Für feingranulare Zugriffskontrolle richtet der Service ein eigenes ACL-Profil auf dem Broker ein. Dies geschieht über HTTP-Aufrufe an die SEMPv2-Konfigurationsschnittstelle des Solace-Brokers. Das ACL-Profil für den Controller erlaubt ausschließlich die für diesen Anwendungsfall nötigen Operationen:
        \begin{enumerate}
            \item Publish-Erlaubnis: Das Gerät darf nur auf seinem eigenen Topic-Pfad Messwerte publizieren. Im Prototoll wird das Topic-Muster sensora/v1/send/{controller\_id} verwendet. Über SEMP wird ein Topic Exception hinzugefügt, die genau dieses Topic für Publish freigibt (alle anderen werden per Default “disallow” gesetzt).
            \item Subscribe-Erlaubnis: Analog erhält das Gerät das Recht, Sollwert-Nachrichten zu abonnieren, die an sein spezifisches Target-Topic gesendet werden. Dies ist typischerweise sensora/v1/receive/{controller\_id}/targetValues. Auch hierfür wird programmgesteuert eine Ausnahme im ACL-Profil hinterlegt. Damit ist sichergestellt, dass der Controller nur Nachrichten empfängt, die explizit an ihn adressiert sind (und z.B. keine fremden Gerätedaten).
        \end{enumerate}
        \item Nachdem ACL-Profil und Berechtigungen erfolgreich angelegt wurden, erstellt der Service über die SEMP-API den Broker-Client-User und weist ihm dieses ACL-Profil zu. Sollte einer der Schritte fehlschlagen (z.B. aufgrund bereits existierender Einträge oder Verbindungsfehler zum Broker), bricht die Funktion ab und der Auth-Vorgang resultiert in einem Server-Fehler (HTTP 500). Im Erfolgsfall erhält der Auth-Service nun ein Credential-Paket bestehend aus Broker-URL, Username und Passwort für den neuen Controller.
    \end{enumerate}
    Die Nutzung der Solace Element Management Protocol API ermöglicht es, die Broker-Konfiguration zu automatisieren. Solace SEMP ist eine RESTful-Schnittstelle\cite{SolaceSEMP}, die das Anlegen von Objekten (Queues, Benutzer, ACLs etc.) per Skript erlaubt. Dadurch wird eine dynamische Inbetriebnahme neuer Geräte ohne manuelle Eingriffe möglich – ein wichtiger Vorteil im IoT-Kontext. Durch entsprechende Fehlerbehandlung (Auswerten des HTTP-Status: 200 Erfolg, 409 „Conflict“ bei bereits vorhandenen Ressourcen) ist die Funktion weitgehend wiederholbar, ohne Inkonsistenzen zu erzeugen. Beispielsweise würde ein erneuter Aufruf für denselben Controller erkennen, dass dessen Benutzerkonto schon existiert (der Auth-Service speichert dies ja auch in seiner config) und überspringt die Neuanlage. So bleibt der Prozess idempotent und ein Gerät könnte die Authentifizierung bei Bedarf erneut durchlaufen, um z.B. verlorene Credentials abzurufen.
    \item Registrierung abschließen und Antwort an Gerät: Zum Abschluss der /verify-Route führt der Auth-Service noch zwei Aktionen aus: (a) Eintrag in der Systemdatenbank: Falls noch nicht geschehen, wird der Controller endgültig in der sensora.controllers-Tabelle der DB vermerkt (inkl. Besitzverknüpfung zum Benutzerkonto, was zuvor im Request mitgesendet wurde). Außerdem kann hier optional ein Default-Sensor für den Controller in der DB angelegt werden (im Code wird bspw. ein Platzhalter-Sensor für Temperatur erstellt, um direkt Messwerte speichern zu können). (b) Secure Credential Delivery: Die vom Broker erzeugten Verbindungsdaten (Username/Passwort, Host) müssen nun dem Gerät mitgeteilt werden. Da diese Angaben sehr sensitiv sind, wurden besondere Maßnahmen getroffen, um sie vertraulich und integer zum Gerät zu übertragen. Der Server generiert zunächst einen einmaligen Session Key (eine randomisierte Byte-Sequenz mittels Fernet.generate\_key()), mit dem er die Credentials symmetrisch verschlüsselt (Fernet nutzt intern AES-128 in GCM-Modus\cite{fernetSpec} mit eingebauter HMAC für Integrität). Die so entstehende Ciphertext-Nutzlast wird als encrypted\_credentials bereitgestellt. Zusätzlich erzeugt der Server einen Credential-HMAC (credential\_key), indem er den Session Key mit dem ursprünglichen Geräte-PSK mittels HMAC-SHA256 signiert. Anschließend sendet der Auth-Service dem Gerät folgende Daten im JSON-Response:
    \begin{enumerate}
        \item session\_key: der im Base64-Format kodierte symmetrische Schlüssel,
        \item credential\_key: der HMAC (Hex-String) zur Absicherung,
        \item encrypted\_credentials: die verschlüsselten Broker-Zugangsdaten (Base64-Text).
    \end{enumerate}
    Diese Konstruktion erlaubt es dem Gerät, die erhaltenen Credentials auf Vertrauenswürdigkeit zu prüfen: Nur wenn es den gleichen HMAC über den Session Key mit seinem PSK berechnet und dieser mit dem credential\_key übereinstimmt, stammen die Daten eindeutig vom Auth-Service (der das PSK kennt). Damit ist ein Schutz gegen Man-in-the-Middle-Angriffe erreicht, selbst wenn kein vollwertiger TLS-Kanal vorhanden wäre – ein Angreifer könnte zwar den Session Key und Ciphertext stehlen, hätte aber ohne PSK keine Möglichkeit, gültige Daten vorzutäuschen. Nach erfolgreicher HMAC-Prüfung entschlüsselt das Gerät mit dem Session Key die Credentials und erhält so seinen persönlichen Broker-Login. Ab diesem Zeitpunkt kann sich der Controller am MQTT-Broker anmelden und regulär Sensordaten austauschen. Der einmalig verwendete Session Key ist nun obsolet.
\end{enumerate}
Abschließend bestätigt der Auth-Service dem Gerät die erfolgreiche Verifikation mit HTTP 200. Intern protokolliert er die erfolgreiche Authentifizierung und der Prozess ist abgeschlossen. Jegliche Fehlersituationen unterwegs (z.B. falscher Token, falsche Response, fehlende Eingabefelder) wurden mit aussagekräftigen HTTP-Codes (400 Bad Request, 403 Forbidden) und Log-Meldungen abgefangen, sodass das Gerät bzw. der Administrator direkt Rückmeldung über den Grund eines Scheiterns erhalten.


\section{Mail-Service: E-Mail-Verifikation von Benutzerkonten}
Der Mail-Service ist ein eigenständiger Webservice, der die Verifizierung von Benutzer-E-Mailadressen übernimmt. Im Gesamtsystem wird dieser Service genutzt, um nach einer Benutzerregistrierung sicherzustellen, dass die angegebene E-Mail dem Nutzer gehört und erreichbar ist – ein gängiges Verfahren, um Kontoaktivierungen durch den Nutzer selbst via Klick auf einen Bestätigungslink durchzuführen. Der Mail-Service wurde mit FastAPI (Python) umgesetzt, was die Erstellung asynchroner HTTP-Handler ermöglicht. Die Hauptaufgaben des Dienstes sind: Empfang der Verifikationsanfrage, Validierung mittels eines Pre-Shared Key (zur Absicherung interner Aufrufe), Generierung eines eindeutigen Bestätigungs-Tokens, Versand einer E-Mail mit Bestätigungslink via SMTP und abschließend die Verarbeitung des Bestätigungs-Clicks (Aktivierung des Kontos in der Datenbank).
\subsection{Architektur und Ablauf der E-Mail-Verifikation}
Der Mail-Service verfügt über zwei wesentliche Endpoints: einen POST-Endpoint /verify zum Anfordern einer Verifikationsmail und einen GET-Endpoint /confirm/{username}/{token} zum Bestätigen. Intern nutzt der Service eine PostgreSQL-Datenbankverbindung (asynchron via asyncpg), um Benutzerdatensätze zu prüfen und zu aktualisieren. Der Ablauf lässt sich wie folgt zusammenfassen:
\begin{enumerate}
    \item Anfrage zur Verifikation (POST /verify): Diese Schnittstelle wird vom übergeordneten System (z.B. dem Web-Frontend oder einem anderen Service) aufgerufen, sobald ein Benutzer eine Registrierung abgeschlossen hat oder eine E-Mail-Bestätigung angefordert wird. Der Request enthält typischerweise den Benutzernamen und die E-Mail-Adresse des Kontos. Zusätzlich erwartet der Service einen geheimen Schlüssel (key), der mitgeschickt wird. Dieser PSK (Mailservice) ist eine einfache Sicherungsmaßnahme, damit nur autorisierte Systeme (etwa das Frontend-Servermodul) den Versand von Verifizierungs-Mails auslösen können – damit wird verhindert, dass Unbefugte massenhaft Verifikations-E-Mails über die öffentliche API triggern. Der Service prüft also zuerst, ob der mitgesandte Schlüssel mit dem in den Umgebungsvariablen hinterlegten Wert (MAILSERVICE\_PSK) übereinstimmt. Ist dies nicht der Fall, wird mit HTTP 403 abgebrochen.
    Ist die Anfrage autorisiert, wird die angegebene Kombination aus username und mail in der Datenbank gesucht (SELECT * FROM sensora.users WHERE username=\%s AND mail=\%s). Nur wenn ein entsprechender Benutzeraccount existiert und noch als inaktiv markiert ist (dies wird indirekt geprüft, indem z.B. ein Feld active in der DB auf FALSE stehen sollte – im Code wird bei Nichtexistenz direkt 404 gemeldet), wird der Verifikationsprozess fortgesetzt. Im nächsten Schritt erzeugt der Service ein zufälliges Token als einmaligen Bestätigungscode. Hierzu wird Python secrets.token\_urlsafe(16) verwendet\cite{pythonSecrets}, was einen ~22 Zeichen langen kryptographisch sicheren String liefert. Das Token wird in einer in-memory Datenstruktur (tokens Dictionary) unter dem Schlüssel des Benutzernamens gespeichert. Anschließend wird ein Bestätigungslink erstellt, der die URL des Confirmation-Endpoints enthält (inkl. Pfadparameter für Username und Token). Dieser Link hat z.B. die Form: https://meinserver/confirm/alice/AbCdEfGh... – er enthält also das geheime Token.
    Nun versendet der Mail-Service eine E-Mail an die Adresse des Nutzers. Dafür wird ein SMTP-Server (hier Gmail SMTP auf Port 587) verwendet. Über Pythons smtplibwird eine TLS-geschützte Verbindung aufgebaut, der Mailaccount authentifiziert (SMTP-User und Passwort liegen in den Settings) und dann eine Textnachricht verschickt. Der E-Mail-Inhalt besteht aus einem kurzen Text mit der Aufforderung, den Link anzuklicken, um die Registrierung abzuschließen. Absender und Betreff sind entsprechend gesetzt (z.B. "Bitte bestätige deine E-Mail"). Nach erfolgreichem Versand gibt der/verify-Endpoint eine Erfolgsmeldung zurück ({"message": "Verification email sent."}mit HTTP 200). Fehlerfälle:
    Wenn die E-Mail-Adresse nicht existiert oder der DB-Zugriff fehlschlägt, wird ein HTTP 404 bzw. 500 zurückgegeben. Ein falscher PSK führt zu 403. Falls der SMTP-Versand scheitert (Exception), wird diese von FastAPI als Serverfehler zurück an den Aufrufer propagiert – in einer robusteren Version könnte man hier spezifisch mitHTTPException` antworten, doch im gegebenen Code wird auf die eingebaute Exception-Behandlung vertraut.
    \item Bestätigungsaufruf (GET /confirm/{username}/{token}): Diese Route wird aufgerufen, wenn der Benutzer den Link in der Verifikationsmail anklickt. In einem üblichen Web-Anwendungsfluss würde dieser Link z.B. zu einer Erfolgsmeldungsseite führen. Der Mail-Service übernimmt hier im Hintergrund die Aktivierung des Benutzerkontos. Er prüft zunächst, ob zum gegebenen username ein Token in seinem Zwischenspeicher vorliegt und ob es mit dem übermittelten Token übereinstimmt. Ist das Token falsch oder nicht (mehr) vorhanden, wird eine HTTP 400 Fehlermeldung erzeugt ("Invalid or expired token."). Dies deckt sowohl falsch manipulierte URLs als auch abgelaufene Tokens ab – letzteres, weil der Service das Token nach Gebrauch löscht oder nach einem Neustart vergisst (siehe weiter unten). Wenn das Token stimmt, wird mittels Datenbank-Update das Benutzerkonto aktiviert (UPDATE sensora.users SET active = TRUE WHERE username = ...). Danach entfernt der Service den genutzten Token aus seinem tokens-Dictionary (damit der Link nicht erneut verwendet werden kann, One-Time Use). Schließlich liefert der Endpoint eine einfache HTML-Antwort zurück, die dem Nutzer bestätigt, dass die E-Mail erfolgreich verifiziert wurde (im Code: Rückgabe eines kleinen <h1>-HTML mit Erfolgstext). Dieses HTML wird durch FastAPI mithilfe der HTMLResponse direkt ausgegeben – so sieht der Nutzer unmittelbar im Browser eine Bestätigung.
\end{enumerate}
Der Mail-Service arbeitet ereignisgetrieben: Nur bei Bedarf wird eine Mail erzeugt, es gibt keinen dauerhaften Hintergrundprozess außer der DB-Verbindung. Durch FastAPI’s asyncio-basierte Architektur \cite{fastAPI}kann der Service viele Anfragen gleichzeitig abwickeln, ohne dass der Versand einer Mail (der einige Sekunden dauern kann) den gesamten Server blockiert. In unserem Fall wird zwar smtplib (synchron) genutzt – was den Event Loop blockiert – doch da der zu erwartende Aufrufdurchsatz gering ist (E-Mails nur bei Registrierung, nicht ständig), wurde auf komplexere nebenläufige Auslagerung verzichtet.

\section{Database Writer: MQTT-Datenpersistierung in PostgreSQL}
Der Database Writer Service ist ein Hintergrunddienst, der eingehende Sensordaten von den Geräten entgegennimmt und diese zuverlässig in der relationalen Datenbank speichert. Er bildet damit das Bindeglied zwischen der Echtzeit-MQTT-Datenebene und der persistenten Speicherung. Aus den Anforderungen geht hervor, dass Messwerte nicht verloren gehen sollen und zeitlich historisiert abrufbar sein müssen. Daher wurde eine Lösung implementiert, die auf nachrichtenbasierten Warteschlangen und garantierter Zustellung basiert. Der Database Writer subscribiert nicht einfach flüchtig auf MQTT-Themen, sondern nutzt den Solace-Broker mit einer persistenten Queue, um eine ausfallsichere Verarbeitung zu gewährleisten.
\subsection{Architektur: Dauerhafter Queue-Consumer}
Im Gegensatz zu den zuvor beschriebenen Webservices läuft der Database Writer ohne HTTP-Schnittstelle – er startet bei Systembeginn und läuft kontinuierlich als Daemon. Implementiert wurde er in Python unter Verwendung der Solace-eigenen Python API (solace.messaging), welche eine JMS-ähnliche Schnittstelle bietet. Die Hauptkomponenten sind:
\begin{enumerate}
    \item Solace-Verbindung: Beim Start baut der Service zunächst eine Verbindung zum Solace PubSub+ Broker auf. Dafür werden die Verbindungsparameter (Host, VPN, Username, Passwort) aus Umgebungsvariablen gelesen. Im Docker-Setup zeigt z.B. SOLACE\_HOST auf den internen Broker (tcp://solace:55555 für non-SSL MQTT über das interne Solace-Protokoll). Der Code versucht bis zu 10 mal in einem Retry-Loop die Verbindung herzustellen, mit Wartezeit, da der Broker evtl. noch am Hochfahren ist. Dieser Mechanismus erhöht die Robustheit: sollte der Broker zum Zeitpunkt des Writer-Starts nicht bereit sein, gibt der Service nicht sofort auf, sondern wartet insgesamt bis zu ~50 Sekunden auf eine erfolgreiche Verbindung.
    \item Persistente Queue und Konsument: Nach Verbindungsaufbau erstellt der Service einen Consumer auf einer durablen Message-Queue namens sensor\_data. Diese Queue ist so konfiguriert, dass sie alle relevanten Sensor-MQTT-Nachrichten aufnimmt. Die Zuordnung erfolgt über Subscriptions, die der Queue im Broker zugewiesen sind (dazu später mehr im Solace-Init Teil). Damit fungiert die Queue als Pufferspeicher: eintreffende MQTT-Publishs der Geräte werden vom Broker auf dieser Warteschlange zwischengespeichert, bis der Database Writer sie abholt. Die Verwendung einer persistenten Queue garantiert, dass keine Daten verlorengehen, selbst wenn der Consumer zwischenzeitlich ausfällt oder Netzwerkprobleme auftreten – der Broker hält die Nachrichten vor. Die Queue ist im Compose-Setup als exclusive deklariert, d.h. sie wird nur von einem Consumer genutzt, was sicherstellt, dass genau ein Service-Exemplar alle Daten chronologisch verarbeitet (kein Load Balancing hier gewünscht). Der Database Writer startet einen asynchronen Empfang auf dieser Queue mittels receiver.receive\_async(MessageHandler()). Hier wird ein benutzerdefinierter MessageHandler (eine Klasse, die eine on\_message-Methode überschreibt) verwendet, was dem Entwurf eines Event-Callbacks entspricht: Jede eingehende Nachricht triggert den Aufruf von SensorMessageHandler.on\_message.
    \item Verarbeitung eingehender Nachrichten: Im on\_message-Callback wird die erhaltene Nachricht zuerst vom proprietären Format in einen String dekodiert und dann als JSON geparst. Die erwartete Struktur der Nachrichten – dies wurde im theoretischen Teil des Datenformats definiert – beinhaltet in der obersten Ebene eine Controller-Kennung (did) und eine Liste von Sensor-Datensätzen (sensors). Jede Sensorstruktur enthält eine Sensor-ID (sid), einen Status (status) und ggf. einen Array von Messwerten (values). Der Database Writer iteriert über alle Sensoren in der Nachricht und führt für jeden folgende Schritte aus:
    \begin{enumerate}
        \item Status-Update (Heartbeat): Unabhängig davon, ob Messwerte vorliegen, wird die Information genutzt, dass ein Sensor Daten gesendet hat. Über die Hilfsfunktion update\_last\_call(sensor\_id, status) wird in der Datenbank der letzte Meldungszeitpunkt (last\_call Timestamp) und der Status des Sensors aktualisiert. Dies dient dazu, die Erreichbarkeit bzw. Aktivität von Sensoren nachzuverfolgen. Im Code wird hierbei ein frischer DB-Verbindungszyklus genutzt: update\_last\_call öffnet eine DB-Verbindung, führt ein UPDATE sensora.sensors SET last\_call = NOW(), status = \%s WHERE sid = \%s, und schließt die Verbindung wieder. Der Status wird auf den vom Gerät gemeldeten Wert gesetzt (typischerweise "active" bei normaler Meldung). Damit implementiert der Service ein Heartbeat-Monitoring: jedes Gerät signalisiert durch Senden (selbst von Messwerten) seine Aktivität.
        \item Messwertspeicherung: Falls der Sensor Messwerte im JSON mitgeliefert hat (values-Array nicht leer), werden diese in der Datenbank persistiert. Hierzu ruft der Handler die Funktion save\_sensor\_data(sensor\_id, values, controller\_id) auf. Innerhalb dieser Routine findet eine detaillierte Behandlung statt:
        \begin{enumerate}
            \item Zunächst wird sichergestellt, dass der referenzierte Controller existiert (Datenintegrität). Dazu wird in der Tabelle sensora.controllers per SELECT geprüft, ob did = controller\_id vorhanden ist. Ist dies nicht der Fall, wird ein Warnhinweis geloggt und die Speicherung für diesen Sensor abgebrochen – das System ignoriert also Messdaten von unbekannten Geräten. Im Normalfall sollten alle Controller aus dem Auth-Service bekannt sein.
            \item Als nächstes wird geprüft, ob der spezifische Sensor bereits in der Datenbank angelegt ist. Die Sensoren sind in der Tabelle sensora.sensors modelliert, mit Primärschlüssel sid. Falls das SELECT ergibt, dass dieser Sensor noch nicht existiert, interpretiert der Service dies als erstmalige Meldung eines neuen Sensors an diesem Controller. In unserem Systemdesign könnten Sensoren dynamisch erkannt werden (z.B. wenn ein Controller ein neues Sensormodul bekommt). In so einem Fall legt der Database Writer automatisch einen neuen Sensor-Datensatz in der DB an. Hierfür entnimmt er der Nachricht, falls vorhanden, Meta-Informationen über den Sensor (im JSON ggf. enthalten unter "sensor\_info"). Im Code wird die erste Value-Nachricht auf sensor\_info geprüft und daraus z.B. der Sensortyp (ilk, z.B. "humidity" oder "temperature") und Einheit (unit, z.B. "\%", "°C") extrahiert. Diese werden zusammen mit der Sensor-ID und der Controller-ID in sensora.sensors eingefügt. Dadurch wird der Sensor dem System bekannt gemacht. Wichtig: Beim Insert wird plant = NULL gesetzt, da initial der Sensor noch keiner Pflanze zugeordnet ist. Nach diesem Insert wird sofort ein commit durchgeführt, damit der neue Sensor auch in weiteren Schritten verfügbar ist. Zudem loggt das System die Anlage des Sensors.
            \item Zuordnungsprüfung: Ein kritischer Aspekt ist, dass Messwerte nur gespeichert werden sollen, wenn klar ist, welcher Pflanze sie zugeordnet sind. Im Datenmodell hat jeder Sensor optional einen Fremdschlüssel auf sensora.plants. Direkt nach dem Insert (oder wenn Sensor schon existierte) wird daher plant\_id aus dem Sensor-Datensatz ausgelesen. Ist plant\_id NULL (Sensor keiner Pflanze zugeordnet), bricht die Funktion ab ohne die Messwerte zu speichern. Dieser Schritt stellt sicher, dass Daten erst dann persistiert werden, wenn die organisatorische Verknüpfung hergestellt wurde –  um zu vermeiden, dass "verwaiste" Messwerte in der Datenbank landen, die keiner Pflanze zugeordnet sind. In der Praxis würde ein Nutzer in der Applikation also zunächst einen Sensor einer Pflanze (Topf) zuweisen, bevor Werte fließen. Nicht zugeordnete Sensoren melden zwar ihren Status (wodurch last\_call aktualisiert wird), aber ihre Werte werden bis zur Zuweisung verworfen (im Code durch Log "Sensor ist keiner Pflanze zugeordnet. Werte werden nicht gespeichert." gekennzeichnet).
            
            \item Werte-Insert: Falls ein plant\_id vorhanden ist, iteriert der Service über alle übermittelten Messwerte im values Array. Jeder Eintrag enthält typischerweise einen Zeitstempel (timestamp) und einen numerischen Wert (value). Wenn kein Timestamp angegeben ist, wird im Code "CURRENT\_TIMESTAMP" als Platzhalter genutzt, was die DB veranlasst, den Einfügezeitpunkt zu nehmen. Für jeden Wert generiert der Service eine eindeutige ID (vid via UUID4) und führt ein INSERT in die Tabelle sensora.values aus. Dabei werden Wert, Timestamp, Sensor-ID und Plant-ID gespeichert. Die Verwendung einer eigenen UUID für jeden Messwert garantiert, dass Einträge eindeutig sind; alternativ hätte man ein Serien-ID der DB nutzen können – hier zeigte sich aber der Designwunsch nach verteilbar eindeutigen IDs (was in IoT-Systemen mit mehreren Quellen sinnvoll sein kann). Nach dem Schleifendurchlauf über alle Werte werden die Insertionen per conn.commit() in der DB finalisiert. Abschließend wird die DB-Verbindung geschlossen. Im Log erscheint dann pro Sensor eine Bestätigung ("Alle Werte für Sensor X gespeichert.").
            \item Fehler während der DB-Operationen werden aufgefangen und als Fehler geloggt, ohne dass der gesamte Service abstürzt. Sollte z.B. während der Inserts ein DB-Fehler auftreten, würde zwar dieser Aufruf fehlschlagen, aber der Message Handler an sich fängt die Exception und beendet nicht den Prozess (siehe nächster Punkt).
        \end{enumerate}
        \item Message Acknowledgement: Nachdem alle Sensoren einer Nachricht verarbeitet wurden, bestätigt der Database Writer dem Broker den erfolgreichen Empfang mittels receiver.ack(message). Dies ist ein wichtiger Schritt im Zusammenspiel mit der persistenten Queue: Erst durch das Acknowledge wird die Nachricht aus der Queue entfernt. Sollte der Service abstürzen oder es käme zu einem nicht behandelten Fehler vor dem Ack, würde die Nachricht in der Queue bleiben und später erneut zugestellt werden können (garantierte mindestens-einmal-Zustellung). Im implementierten Code wird das Ack nur im erfolgreichen JSON-Verarbeitungsfall aufgerufen. Bei bestimmten Fehlern, z.B. JSON-Parsing-Error oder falls essentielle Felder fehlen, wird kein Ack gesendet, was bedeutet, dass die Nachricht in der Queue verbleibt. Im Log wird ein Hinweis ausgegeben ("Nachricht ist kein gültiges JSON" oder "Ungültige Nachricht. Controller-ID fehlt."), aber ein Ack fehlt. Dieses Verhalten könnte zu einer erneuten Zustellung führen (je nach Broker-Einstellung) oder die Nachricht blockiert die Queue. Im aktuellen Setup von Solace würde eine nicht bestätigte Nachricht in einer durable Queue verbleiben; der Consumer könnte z.B. nach einem Timeout neu gestartet werden, um es erneut zu versuchen. Hier wäre eventuell eine Verbesserung, solche Nachrichten nach x Versuchen in eine Dead Message Queue zu verschieben – jedoch ist dies im Code nicht implementiert. Somit wird sich darauf verlassen, dass gut formatierte Nachrichten ankommen. Die bewusste Entscheidung, bei unverarbeitbaren Nachrichten kein Ack zu senden, spiegelt einen Anspruch auf Datenintegrität: Lieber bleibt eine fehlerhafte Nachricht liegen (und ein Admin greift ein), als dass sie fälschlich als verarbeitet markiert wird.

    \end{enumerate}
\end{enumerate}
Zusätzlich zur Callback-Verarbeitung hat der Database Writer einen Nebenprozess zur Überwachung der Sensor-Aktivität:
Timeout-Überprüfung: Mithilfe eines einfachen Endlosschleife-Timers im Hauptthread wird in regelmäßigen Abständen (jede Minute, gemäß CHECK\_INTERVAL = 60 Sekunden) die Funktion check\_sensor\_timeouts() ausgeführt. Diese öffnet eine DB-Verbindung und führt ein Update auf der Sensors-Tabelle aus, um alle Sensoren, deren last\_call älter als 5 Minuten ist, auf Status 'error' zu setzen. Damit wird ein Timeout-Mechanismus realisiert: Wenn ein Sensor 5 Minuten lang keine Daten gesendet hat (und vorher auf 'active' stand), gilt er als potenziell offline oder ausgefallen. Im Datenbankmodell wird dies durch status = 'error' kenntlich gemacht. Dieser Mechanismus ergänzt das oben erwähnte Heartbeat-Tracking. Der Zähler wird nach jedem Lauf zurückgesetzt. Durch die Schleife mit time.sleep(1) wird der CPU-Verbrauch minimal gehalten.

\section{Setpoint API: Sollwert-Vorgabe via REST und MQTT}
Die Setpoint API ermöglicht es, aus dem System heraus Steuerungswerte an die Mikrocontroller zu senden – konkret Sollwerte für bestimmte Sensoren (z.B. Feuchtigkeits-Sollwert für die Bewässerungssteuerung). Damit wird das System bidirektional: Nicht nur melden Sensoren Zustände, sondern Aktoren können angesteuert werden. Der Dienst ist als kleiner Flask-basierter Webservice realisiert, der eine REST-Endpoint bereitstellt, über den ein Sollwert gesetzt werden kann. Intern publiziert der Service diesen Sollwert dann als MQTT-Nachricht auf den Broker, sodass der Ziel-Controller ihn empfängt. Im Grunde handelt es sich also um einen Protokollübergang von HTTP zu MQTT.
\subsection{Funktionsweise und Ablauf}
Der Setpoint-Service stellt den Endpoint /sollwert (HTTP POST) bereit. Der typische Ablauf, um einen Sollwert zu setzen, ist:
\begin{enumerate}
    \item Eine externe Entität – z.B. eine Web-Frontend-Anwendung oder ein Benutzer via App – sendet einen HTTP-POST an die Setpoint API mit den Parametern:
    \begin{enumerate}
        \item controller\_id: die Kennung des Ziel-Controllers (Gerät),
        \item sensor\_id: die Kennung des Sensors (bzw. des Aktors) auf diesem Gerät, für den der Sollwert gelten soll,
        \item sollwert: der anzustrebende Wert (numerisch, z.B. Feuchtigkeit in \% oder ein Schwellwert).
    \end{enumerate}
    \item Die API prüft eingehend, ob alle nötigen Felder vorhanden und gültig sind. Falls etwas fehlt, wird mit HTTP 400 Bad Request geantwortet und ein Fehlerjson zurückgegeben.
    \item Ist die Eingabe valide, generiert der Service eine MQTT-Nachricht. Dazu wird ein JSON-Objekt erstellt, das wie folgt aussieht:
    \begin{lstlisting}
    {
        "targetValues": [
            {
                "did": "<controller_id>",
                "sid": "<sensor_id>",
                "value": <sollwert>
            }
        ]
    }
    \end{lstlisting}
    Diese Struktur ist angelehnt an das Format, das ggf. vom Gerät erwartet wird (eine Liste von Zielwerten für bestimmte Sensoren/Aktoren). Es wird also der Device-ID und Sensor-ID nochmals eingebettet, damit das Gerät die Nachricht zuordnen kann.
    \item Als nächstes bestimmt der Service das Ziel-Topic für die MQTT-Nachricht. Gemäß der in Auth-Service eingerichteten Konvention wird das Topic sensora/v1/receive/<controller\_id>/targetValues genutzt. Darauf ist der betreffende Controller (bzw. dessen MQTT-Client) berechtigt zu lauschen. Dieses Topic adressiert somit genau den gewünschten Controller.
    \item Der Service publiziert die Nachricht über den Solace-Broker: Hierzu nutzt er die vorab aufgebaute Broker-Verbindung und einen Publisher, der als persistent message publisher initialisiert wurde. Die Nachricht wird mit dem oben genannten Topic abgesendet. Der Broker sorgt dann dafür, dass – sofern der Controller online ist und das Topic abonniert hat – die Nachricht an diesen zugestellt wird. Sollte der Controller momentan nicht verbunden sein, greift je nach Broker-Einstellung die Persistierung: Entweder wurde eine Queue für solche Sollwert-Nachrichten eingerichtet (vgl. mögliches sensor\_setpoints Queue), oder im MQTT-Kontext übernimmt der Broker das Speichern bei QoS>0. Da der Publisher hier als "persistent" konfiguriert ist, lässt sich ableiten, dass die Nachricht als durable versendet wird, was in MQTT-Terminologie etwa QoS 1 entspricht (mindestens einmal zustellen). Solace bietet für MQTT-Clients mit dauerhafter Session auch an, solche Nachrichten zwischenzuspeichern, was vermutlich hier genutzt wird.
    \item Die Setpoint API gibt dem HTTP-Aufrufer eine Erfolgsmeldung zurück (HTTP 200, JSON {"status": "success"}). Damit ist der Vorgang für den Benutzer abgeschlossen. Im Hintergrund allerdings wird nun der Controller die Nachricht empfangen und z.B. seine Konfiguration anpassen (dies ist Teil der Geräte-Firmware und außerhalb dieses Service-Scopes, aber essentiell für den Regelkreis der Bewässerung).
\end{enumerate}

\section{Solace Init: Automatisierte Broker-Konfiguration}
Der Solace Init Service (bzw. Skript) wurde implementiert, um bei Start des Gesamtsystems sicherzustellen, dass der Solace-Broker über alle notwendigen Persistent Queues und Topic-Weiterleitungen verfügt. Er stellt somit eine Infrastruktur-Komponente dar, die eng mit dem Broker zusammenarbeitet. Da in einem Container-Setup der Broker beim ersten Start völlig jungfräulich ist, muss z.B. die sensor\_data Queue angelegt und mit dem entsprechenden Topic verbunden werden. Solace Init erfüllt genau diese Aufgabe über die SEMP v2 Management-API.
\subsection{Vorgehen und Konfiguration}
Solace Init ist als eigenständiges Python-Skript (init.py) konzipiert, das beim Hochfahren des Docker-Compose Stacks einmalig ausgeführt wird und sich danach beendet. Seine Aufgaben sind:
\begin{enumerate}
    \item Einlesen der gewünschten Queue-Konfiguration: Aus einer JSON-Datei (im Code queues.json) werden alle zu erstellenden Queues und ihre Eigenschaften/Subscriptions geladen. Dieses File definiert quasi deklarativ, welche Warteschlangen mit welchen Parametern und Abos existieren sollen. Ein beispielhafter Inhalt könnte so aussehen:
    \begin{lstlisting}
        
    {
      "queues": [
        {
          "queueName": "sensor_data",
          "accessType": "exclusive",
          "egressEnabled": true,
          "ingressEnabled": true,
          "subscriptions": [
            { "subscriptionTopic": "sensora/v1/send/>" }
          ]
        },
        {
          "queueName": "sensor_setpoints",
          "accessType": "exclusive",
          "subscriptions": [
            { "subscriptionTopic": "sensora/v1/receive/>" }
          ]
        }
      ]
    }
    \end{lstlisting}
    Hier würde z.B. festgelegt, dass es eine Queue sensor\_data gibt, die exklusiv ist und sowohl Ingress/Egress aktiviert hat (Standard für persistent Queues), und dass sie alle Topics unter sensora/v1/send abonniert (der > Wildcard deckt alle Unterpfade ab). Ebenso eine Queue sensor\_setpoints für alle sensora/v1/receive Nachrichten. Dieses Konstrukt deckt die zuvor diskutierten Pfade ab: Messwerte und Steuerbefehle.
    \item Verbindung zur SEMP API: Solace Init nutzt Python requests, um HTTP-POSTs an den Broker zu senden. Die notwendigen Zugangsdaten (Admin-User, Passwort, Host:Port) werden aus Env-Variablen bezogen (SOLACE\_USER, SOLACE\_PASS, SOLACE\_HOST\_SEMP). Im Docker Compose sieht man, dass solace-init Container mit depends\_on: solace gestartet wird und erst nach ~80s (wenn Broker läuft) das Skript ausführt. So ist sichergestellt, dass der Broker Management-Port 8080 erreichbar ist. SEMP v2 erfordert Basic-Auth mit dem Admin-Login\cite{solaceSEMP}, was hier als Tuple im requests.post(..., auth=(user, pass)) genutzt wird.
    \item Erstellen von Queues (SEMP /config): Für jede in der JSON gelistete Queue baut das Skript die entsprechende URL und das Payload zusammen. Beispiel: POST http://solace:8080/SEMP/v2/config/msgVpns/default/queues mit JSON 
    \begin{lstlisting}
        {"queueName": "sensor_data", "accessType": "exclusive", "egressEnabled": true, ...}
    \end{lstlisting}.
    Der Broker antwortet mit Status 201 (Created) bei Erfolg. Das Skript prüft den Statuscode:
    \begin{enumerate}
        \item 200/201 wird als Erfolg gewertet und entsprechend geloggt ("Queue erfolgreich erstellt").
        \item 409 (Conflict) bedeutet, die Queue existiert bereits – in diesem Fall loggt das Skript eine Warnung, fährt aber fort. Das ist gewollt, um Idempotenz zu erreichen: Sollte man Solace Init versehentlich zweimal ausführen oder den Broker bereits vorkonfiguriert haben, bricht es nicht ab, sondern erkennt vorhandene Entities.
        \item Andere Fehlercode führt zu einem Fehlerausdruck und Rückkehr aus der Funktion (somit würde die Subscription-Anlage für diese Queue übersprungen).
    \end{enumerate}
    \item Hinzufügen von Topic-Subscriptions: Nach dem Anlegen (oder Erkennen) der Queue iteriert das Skript über alle vorgesehenen Subscription-Themen und sendet für jedes ein POST .../queues/{queueName}/subscriptions mit {"subscriptionTopic": "<topic>"}. Hier gelten ähnliche Statuscodes:
    \begin{enumerate}
        \item 200/201: Subscription hinzugefügt (Logausgabe),
        \item 409: bereits vorhanden (Warnhinweis, aber kein Abbruch),
        \item andere: Fehler ausgeben.
    \end{enumerate}
    \item Abschluss: Nachdem alle Queues aus der Liste abgearbeitet sind, gibt das Skript eine Meldung aus, dass alle Queues und Subscriptions verarbeitet wurden, und endet.
\end{enumerate}
